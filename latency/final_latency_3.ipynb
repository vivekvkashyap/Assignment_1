{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-11 09:46:58,367] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ece/anaconda3/envs/tf-gpu-211/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/ece/anaconda3/envs/tf-gpu-211/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "Flash attention 2 is not installed\n"
     ]
    }
   ],
   "source": [
    "import deepspeed\n",
    "from parler_tts import ParlerTTSForConditionalGeneration\n",
    "from transformers import AutoTokenizer\n",
    "import soundfile as sf\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "/home/system4/anaconda3/envs/tf-gpu-211/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "Config of the text_encoder: <class 'transformers.models.t5.modeling_t5.T5EncoderModel'> is overwritten by shared text_encoder config: T5Config {\n",
      "  \"_name_or_path\": \"google/flan-t5-base\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "Config of the audio_encoder: <class 'parler_tts.dac_wrapper.modeling_dac.DACModel'> is overwritten by shared audio_encoder config: DACConfig {\n",
      "  \"_name_or_path\": \"ylacombe/dac_44khZ_8kbps\",\n",
      "  \"architectures\": [\n",
      "    \"DACModel\"\n",
      "  ],\n",
      "  \"codebook_size\": 1024,\n",
      "  \"frame_rate\": 86,\n",
      "  \"latent_dim\": 1024,\n",
      "  \"model_bitrate\": 8,\n",
      "  \"model_type\": \"dac_on_the_hub\",\n",
      "  \"num_codebooks\": 9,\n",
      "  \"sampling_rate\": 44100,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'parler_tts.modeling_parler_tts.ParlerTTSForCausalLM'> is overwritten by shared decoder config: ParlerTTSDecoderConfig {\n",
      "  \"_name_or_path\": \"/fsx/yoach/tmp/artefacts/decoder_400M/\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"architectures\": [\n",
      "    \"ParlerTTSForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1025,\n",
      "  \"codebook_weights\": null,\n",
      "  \"cross_attention_implementation_strategy\": null,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 1024,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_factor\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"parler_tts_decoder\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codebooks\": 9,\n",
      "  \"num_cross_attention_key_value_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"pad_token_id\": 1024,\n",
      "  \"rope_embeddings\": false,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"scale_embedding\": false,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_fused_lm_heads\": false,\n",
      "  \"vocab_size\": 1088\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_id = \"parler-tts/parler_tts_mini_v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = ParlerTTSForConditionalGeneration.from_pretrained(model_id).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Hello world.\",\n",
    "    \"A futuristic AI assistant responding in a clear, robotic yet friendly manner\",\n",
    "    \"A dramatic voice-over for an action-packed movie trailer that builds suspense\",\n",
    "    \"A casual and friendly conversation starter that feels natural and engaging\",\n",
    "    \"An enchanting introduction to a bedtime story that sparks imagination\",\n",
    "    \"A detailed weather update that provides temperature, wind speed, and overall forecast\",\n",
    "    \"A robotic voice delivering a monotone yet precise system status update\",\n",
    "    \"A powerful and energetic motivational speech that inspires action and confidence\"\n",
    "]\n",
    "\n",
    "descriptions = [\n",
    "    \"A female speaker delivers a warm and welcoming message with a slightly expressive and friendly tone. The speech has a moderate pace and a natural intonation, making it feel inviting. The recording is clear, with minimal background noise.\",\n",
    "    \"A robotic AI voice speaks in a neutral yet slightly friendly manner. The speech is steady, with a consistent pitch and minimal variation in tone. The audio is crisp, resembling a synthesized assistant's response.\",\n",
    "    \"A deep male voice narrates with a dramatic and intense tone, building suspense with pauses and rising intensity. The speech is slow-paced, emphasizing key moments. The recording is cinematic, with a slight reverberation for a grand effect.\",\n",
    "    \"A young adult male speaker talks in a casual and relaxed tone. The speech is natural, with slight variations in pitch and pauses that mimic real-life conversations. The recording is high quality, making it feel like a personal chat.\",\n",
    "    \"A soft-spoken female speaker introduces a bedtime story with a soothing and melodic tone. The pace is slow and gentle, making it easy to follow. The recording has a slight warmth, resembling a close-up microphone capture.\",\n",
    "    \"A professional male voice delivers a clear and informative weather update. The tone is neutral but engaging, with a moderate pace. The articulation is precise, and the recording is high quality, with no distortions.\",\n",
    "    \"A synthetic robotic voice speaks in a completely monotone and even-paced manner. The pitch remains constant, with no emotional inflection. The recording is crisp and clean, resembling an automated system response.\",\n",
    "    \"A dynamic and energetic male speaker delivers an inspiring speech with a strong, enthusiastic tone. The pace is varied, with emphasis on key words to motivate the listener. The recording is sharp and immersive, with no background noise.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(descriptions, return_tensors=\"pt\",padding=\"max_length\",max_length=70).to(device)\n",
    "prompt_input_ids = tokenizer(prompts, return_tensors=\"pt\",padding=\"max_length\",max_length=70).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 took 18.21 seconds\n",
      "Iteration 2 took 19.98 seconds\n",
      "Iteration 3 took 33.39 seconds\n",
      "Iteration 4 took 16.33 seconds\n",
      "Iteration 5 took 14.87 seconds\n",
      "\n",
      "Average execution time: 20.56 seconds\n"
     ]
    }
   ],
   "source": [
    "times = []\n",
    "for i in range(5):\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        generation = model.generate(\n",
    "            input_ids=input_ids.input_ids,\n",
    "            attention_mask=input_ids.attention_mask,\n",
    "            prompt_input_ids=prompt_input_ids.input_ids,\n",
    "            prompt_attention_mask=prompt_input_ids.attention_mask,\n",
    "        )\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    times.append(execution_time)\n",
    "    print(f\"Iteration {i+1} took {execution_time:.2f} seconds\")\n",
    "\n",
    "avg_time = sum(times) / len(times)\n",
    "print(f\"\\nAverage execution time: {avg_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latency Optimization - 1 (Using Enable CUDA Graphs and float16 with caching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_config = {\n",
    "    \"tensor_parallel\": {\n",
    "        \"tp_size\": 1\n",
    "    },\n",
    "    \"dtype\": torch.float16,\n",
    "    \"replace_with_kernel_inject\": True,\n",
    "    \"enable_cuda_graph\": True,}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-10 19:35:31,559] [INFO] [logging.py:128:log_dist] [Rank -1] DeepSpeed info: version=0.16.4, git-hash=unknown, git-branch=unknown\n",
      "[2025-03-10 19:35:31,563] [INFO] [logging.py:128:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1\n"
     ]
    }
   ],
   "source": [
    "engine = deepspeed.init_inference(\n",
    "    model=model,\n",
    "    config=ds_config,\n",
    "    dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 took 16.62 seconds\n",
      "Iteration 2 took 17.27 seconds\n",
      "Iteration 3 took 17.99 seconds\n",
      "Iteration 4 took 19.84 seconds\n",
      "Iteration 5 took 18.74 seconds\n",
      "\n",
      "Average execution time: 18.09 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "times = []\n",
    "for i in range(5):\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        generation = engine.module.generate(\n",
    "            input_ids=input_ids.input_ids,\n",
    "            attention_mask=input_ids.attention_mask,\n",
    "            prompt_input_ids=prompt_input_ids.input_ids,\n",
    "            prompt_attention_mask=prompt_input_ids.attention_mask,\n",
    "            do_sample=True,\n",
    "            use_cache=True,\n",
    "            return_dict_in_generate=True\n",
    "        )\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    times.append(execution_time)\n",
    "    print(f\"Iteration {i+1} took {execution_time:.2f} seconds\")\n",
    "\n",
    "avg_time = sum(times) / len(times)\n",
    "print(f\"\\nAverage execution time: {avg_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latency Optimization - 2 (Using Enable CUDA Graphs and float32 with caching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_config = {\n",
    "    \"tensor_parallel\": {\n",
    "        \"tp_size\": 1\n",
    "    },\n",
    "    \"replace_with_kernel_inject\": True,\n",
    "    \"enable_cuda_graph\": True,}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-10 19:43:08,468] [INFO] [logging.py:128:log_dist] [Rank -1] DeepSpeed info: version=0.16.4, git-hash=unknown, git-branch=unknown\n",
      "[2025-03-10 19:43:08,472] [INFO] [logging.py:128:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1\n"
     ]
    }
   ],
   "source": [
    "engine = deepspeed.init_inference(\n",
    "    model=model,\n",
    "    config=ds_config,\n",
    "    dtype=torch.float32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 took 21.40 seconds\n",
      "Iteration 2 took 18.71 seconds\n",
      "Iteration 3 took 24.90 seconds\n",
      "Iteration 4 took 17.87 seconds\n",
      "Iteration 5 took 16.58 seconds\n",
      "\n",
      "Average execution time: 19.89 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "times = []\n",
    "for i in range(5):\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        generation = engine.module.generate(\n",
    "            input_ids=input_ids.input_ids,\n",
    "            attention_mask=input_ids.attention_mask,\n",
    "            prompt_input_ids=prompt_input_ids.input_ids,\n",
    "            prompt_attention_mask=prompt_input_ids.attention_mask,\n",
    "            do_sample=True,\n",
    "            use_cache=True,\n",
    "            return_dict_in_generate=True\n",
    "        )\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    times.append(execution_time)\n",
    "    print(f\"Iteration {i+1} took {execution_time:.2f} seconds\")\n",
    "\n",
    "avg_time = sum(times) / len(times)\n",
    "print(f\"\\nAverage execution time: {avg_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latency Optimization - 3 (Using Triton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_config = {\n",
    "    \"tensor_parallel\": {\n",
    "        \"tp_size\": 1\n",
    "    },\n",
    "    \"dtype\": torch.float16,\n",
    "    \"replace_with_kernel_inject\": True,\n",
    "    \"enable_cuda_graph\": True,\n",
    "    \"use_triton\": True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-10 19:49:26,669] [INFO] [logging.py:128:log_dist] [Rank -1] DeepSpeed info: version=0.16.4, git-hash=unknown, git-branch=unknown\n",
      "[2025-03-10 19:49:26,673] [INFO] [logging.py:128:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1\n"
     ]
    }
   ],
   "source": [
    "engine = deepspeed.init_inference(\n",
    "    model=model,\n",
    "    config=ds_config,\n",
    "    dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 took 17.03 seconds\n",
      "Iteration 2 took 19.48 seconds\n",
      "Iteration 3 took 16.12 seconds\n",
      "Iteration 4 took 21.26 seconds\n",
      "Iteration 5 took 19.66 seconds\n",
      "\n",
      "Average execution time: 18.71 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "times = []\n",
    "for i in range(5):\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        generation = engine.module.generate(\n",
    "            input_ids=input_ids.input_ids,\n",
    "            attention_mask=input_ids.attention_mask,\n",
    "            prompt_input_ids=prompt_input_ids.input_ids,\n",
    "            prompt_attention_mask=prompt_input_ids.attention_mask,\n",
    "            do_sample=True,\n",
    "            use_cache=True,\n",
    "            return_dict_in_generate=True\n",
    "        )\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    times.append(execution_time)\n",
    "    print(f\"Iteration {i+1} took {execution_time:.2f} seconds\")\n",
    "\n",
    "avg_time = sum(times) / len(times)\n",
    "print(f\"\\nAverage execution time: {avg_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latency Optimization - 4 (Using fp16 and triton autotune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_config = {\n",
    "    \"tensor_parallel\": {\n",
    "        \"tp_size\": 1\n",
    "    },\n",
    "    \"dtype\": torch.float16,\n",
    "    \"replace_with_kernel_inject\": True,\n",
    "    \"enable_cuda_graph\": True,\n",
    "    \"use_triton\": True,\n",
    "    \"triton_autotune\": True,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-11 00:27:18,881] [INFO] [logging.py:128:log_dist] [Rank -1] DeepSpeed info: version=0.16.4, git-hash=unknown, git-branch=unknown\n",
      "[2025-03-11 00:27:18,885] [INFO] [logging.py:128:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1\n"
     ]
    }
   ],
   "source": [
    "engine = deepspeed.init_inference(\n",
    "    model=model,\n",
    "    config=ds_config,\n",
    "    dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 took 15.97 seconds\n",
      "Iteration 2 took 19.79 seconds\n",
      "Iteration 3 took 17.64 seconds\n",
      "Iteration 4 took 16.72 seconds\n",
      "Iteration 5 took 17.76 seconds\n",
      "\n",
      "Average execution time: 17.57 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "times = []\n",
    "for i in range(5):\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        generation = engine.module.generate(\n",
    "            input_ids=input_ids.input_ids,\n",
    "            attention_mask=input_ids.attention_mask,\n",
    "            prompt_input_ids=prompt_input_ids.input_ids,\n",
    "            prompt_attention_mask=prompt_input_ids.attention_mask,\n",
    "            do_sample=True,\n",
    "            use_cache=True,\n",
    "            return_dict_in_generate=True\n",
    "        )\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    times.append(execution_time)\n",
    "    print(f\"Iteration {i+1} took {execution_time:.2f} seconds\")\n",
    "\n",
    "avg_time = sum(times) / len(times)\n",
    "print(f\"\\nAverage execution time: {avg_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_config = {\n",
    "        \"zero_optimization\": {\n",
    "            \"stage\": 3,\n",
    "            \"offload_optimizer\": {\n",
    "                \"device\": \"cpu\",\n",
    "                \"pin_memory\": True\n",
    "            },\n",
    "            \"overlap_comm\": True,\n",
    "            \"reduce_bucket_size\": \"auto\",\n",
    "            \"stage3_prefetch_bucket_size\": \"auto\",\n",
    "            \"stage3_param_persistence_threshold\": \"auto\",\n",
    "            \"stage3_gather_16bit_weights_on_model_save\": True\n",
    "        },\n",
    "        \"train_batch_size\": 8,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.integrations import HfDeepSpeedConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "hfdsc = HfDeepSpeedConfig(ds_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-11 00:32:46,427] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.4, git-hash=unknown, git-branch=unknown\n",
      "[2025-03-11 00:32:46,431] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 1\n",
      "[2025-03-11 00:32:46,471] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2025-03-11 00:32:46,478] [INFO] [logging.py:128:log_dist] [Rank 0] Creating ZeRO Offload\n",
      "[2025-03-11 00:32:46,789] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\n",
      "[2025-03-11 00:32:46,791] [INFO] [utils.py:782:see_memory_usage] MA 2.59 GB         Max_MA 2.61 GB         CA 4.2 GB         Max_CA 4 GB \n",
      "[2025-03-11 00:32:46,793] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 39.99 GB, percent = 32.1%\n",
      "Parameter Offload: Total persistent parameters: 1254898 in 439 params\n",
      "[2025-03-11 00:32:47,150] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\n",
      "[2025-03-11 00:32:47,152] [INFO] [utils.py:782:see_memory_usage] MA 2.59 GB         Max_MA 2.59 GB         CA 4.2 GB         Max_CA 4 GB \n",
      "[2025-03-11 00:32:47,153] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 39.99 GB, percent = 32.1%\n",
      "[2025-03-11 00:32:47,158] [INFO] [config.py:1001:print] DeepSpeedEngine configuration:\n",
      "[2025-03-11 00:32:47,160] [INFO] [config.py:1005:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2025-03-11 00:32:47,161] [INFO] [config.py:1005:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}\n",
      "[2025-03-11 00:32:47,162] [INFO] [config.py:1005:print]   amp_enabled .................. False\n",
      "[2025-03-11 00:32:47,164] [INFO] [config.py:1005:print]   amp_params ................... False\n",
      "[2025-03-11 00:32:47,165] [INFO] [config.py:1005:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2025-03-11 00:32:47,167] [INFO] [config.py:1005:print]   bfloat16_enabled ............. False\n",
      "[2025-03-11 00:32:47,168] [INFO] [config.py:1005:print]   bfloat16_immediate_grad_update  False\n",
      "[2025-03-11 00:32:47,169] [INFO] [config.py:1005:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2025-03-11 00:32:47,171] [INFO] [config.py:1005:print]   checkpoint_tag_validation_enabled  True\n",
      "[2025-03-11 00:32:47,172] [INFO] [config.py:1005:print]   checkpoint_tag_validation_fail  False\n",
      "[2025-03-11 00:32:47,173] [INFO] [config.py:1005:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f69bc2a5c90>\n",
      "[2025-03-11 00:32:47,175] [INFO] [config.py:1005:print]   communication_data_type ...... None\n",
      "[2025-03-11 00:32:47,176] [INFO] [config.py:1005:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2025-03-11 00:32:47,177] [INFO] [config.py:1005:print]   curriculum_enabled_legacy .... False\n",
      "[2025-03-11 00:32:47,178] [INFO] [config.py:1005:print]   curriculum_params_legacy ..... False\n",
      "[2025-03-11 00:32:47,179] [INFO] [config.py:1005:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2025-03-11 00:32:47,180] [INFO] [config.py:1005:print]   data_efficiency_enabled ...... False\n",
      "[2025-03-11 00:32:47,181] [INFO] [config.py:1005:print]   dataloader_drop_last ......... False\n",
      "[2025-03-11 00:32:47,182] [INFO] [config.py:1005:print]   disable_allgather ............ False\n",
      "[2025-03-11 00:32:47,183] [INFO] [config.py:1005:print]   dump_state ................... False\n",
      "[2025-03-11 00:32:47,184] [INFO] [config.py:1005:print]   dynamic_loss_scale_args ...... None\n",
      "[2025-03-11 00:32:47,185] [INFO] [config.py:1005:print]   eigenvalue_enabled ........... False\n",
      "[2025-03-11 00:32:47,186] [INFO] [config.py:1005:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2025-03-11 00:32:47,187] [INFO] [config.py:1005:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2025-03-11 00:32:47,188] [INFO] [config.py:1005:print]   eigenvalue_layer_num ......... 0\n",
      "[2025-03-11 00:32:47,189] [INFO] [config.py:1005:print]   eigenvalue_max_iter .......... 100\n",
      "[2025-03-11 00:32:47,190] [INFO] [config.py:1005:print]   eigenvalue_stability ......... 1e-06\n",
      "[2025-03-11 00:32:47,190] [INFO] [config.py:1005:print]   eigenvalue_tol ............... 0.01\n",
      "[2025-03-11 00:32:47,191] [INFO] [config.py:1005:print]   eigenvalue_verbose ........... False\n",
      "[2025-03-11 00:32:47,192] [INFO] [config.py:1005:print]   elasticity_enabled ........... False\n",
      "[2025-03-11 00:32:47,193] [INFO] [config.py:1005:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2025-03-11 00:32:47,194] [INFO] [config.py:1005:print]   fp16_auto_cast ............... None\n",
      "[2025-03-11 00:32:47,195] [INFO] [config.py:1005:print]   fp16_enabled ................. False\n",
      "[2025-03-11 00:32:47,196] [INFO] [config.py:1005:print]   fp16_master_weights_and_gradients  False\n",
      "[2025-03-11 00:32:47,196] [INFO] [config.py:1005:print]   global_rank .................. 0\n",
      "[2025-03-11 00:32:47,197] [INFO] [config.py:1005:print]   grad_accum_dtype ............. None\n",
      "[2025-03-11 00:32:47,198] [INFO] [config.py:1005:print]   gradient_accumulation_steps .. 1\n",
      "[2025-03-11 00:32:47,199] [INFO] [config.py:1005:print]   gradient_clipping ............ 0.0\n",
      "[2025-03-11 00:32:47,204] [INFO] [config.py:1005:print]   gradient_predivide_factor .... 1.0\n",
      "[2025-03-11 00:32:47,205] [INFO] [config.py:1005:print]   graph_harvesting ............. False\n",
      "[2025-03-11 00:32:47,206] [INFO] [config.py:1005:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2025-03-11 00:32:47,207] [INFO] [config.py:1005:print]   initial_dynamic_scale ........ 65536\n",
      "[2025-03-11 00:32:47,207] [INFO] [config.py:1005:print]   load_universal_checkpoint .... False\n",
      "[2025-03-11 00:32:47,208] [INFO] [config.py:1005:print]   loss_scale ................... 0\n",
      "[2025-03-11 00:32:47,209] [INFO] [config.py:1005:print]   memory_breakdown ............. False\n",
      "[2025-03-11 00:32:47,210] [INFO] [config.py:1005:print]   mics_hierarchial_params_gather  False\n",
      "[2025-03-11 00:32:47,210] [INFO] [config.py:1005:print]   mics_shard_size .............. -1\n",
      "[2025-03-11 00:32:47,211] [INFO] [config.py:1005:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')\n",
      "[2025-03-11 00:32:47,212] [INFO] [config.py:1005:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2025-03-11 00:32:47,213] [INFO] [config.py:1005:print]   optimizer_legacy_fusion ...... False\n",
      "[2025-03-11 00:32:47,214] [INFO] [config.py:1005:print]   optimizer_name ............... None\n",
      "[2025-03-11 00:32:47,215] [INFO] [config.py:1005:print]   optimizer_params ............. None\n",
      "[2025-03-11 00:32:47,215] [INFO] [config.py:1005:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2025-03-11 00:32:47,216] [INFO] [config.py:1005:print]   pld_enabled .................. False\n",
      "[2025-03-11 00:32:47,217] [INFO] [config.py:1005:print]   pld_params ................... False\n",
      "[2025-03-11 00:32:47,218] [INFO] [config.py:1005:print]   prescale_gradients ........... False\n",
      "[2025-03-11 00:32:47,218] [INFO] [config.py:1005:print]   scheduler_name ............... None\n",
      "[2025-03-11 00:32:47,219] [INFO] [config.py:1005:print]   scheduler_params ............. None\n",
      "[2025-03-11 00:32:47,219] [INFO] [config.py:1005:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2025-03-11 00:32:47,220] [INFO] [config.py:1005:print]   sparse_attention ............. None\n",
      "[2025-03-11 00:32:47,221] [INFO] [config.py:1005:print]   sparse_gradients_enabled ..... False\n",
      "[2025-03-11 00:32:47,221] [INFO] [config.py:1005:print]   steps_per_print .............. None\n",
      "[2025-03-11 00:32:47,222] [INFO] [config.py:1005:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False\n",
      "[2025-03-11 00:32:47,223] [INFO] [config.py:1005:print]   timers_config ................ enabled=True synchronized=True\n",
      "[2025-03-11 00:32:47,224] [INFO] [config.py:1005:print]   train_batch_size ............. 8\n",
      "[2025-03-11 00:32:47,226] [INFO] [config.py:1005:print]   train_micro_batch_size_per_gpu  8\n",
      "[2025-03-11 00:32:47,227] [INFO] [config.py:1005:print]   use_data_before_expert_parallel_  False\n",
      "[2025-03-11 00:32:47,227] [INFO] [config.py:1005:print]   use_node_local_storage ....... False\n",
      "[2025-03-11 00:32:47,228] [INFO] [config.py:1005:print]   wall_clock_breakdown ......... False\n",
      "[2025-03-11 00:32:47,229] [INFO] [config.py:1005:print]   weight_quantization_config ... None\n",
      "[2025-03-11 00:32:47,229] [INFO] [config.py:1005:print]   world_size ................... 1\n",
      "[2025-03-11 00:32:47,230] [INFO] [config.py:1005:print]   zero_allow_untested_optimizer  False\n",
      "[2025-03-11 00:32:47,231] [INFO] [config.py:1005:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False\n",
      "[2025-03-11 00:32:47,232] [INFO] [config.py:1005:print]   zero_enabled ................. True\n",
      "[2025-03-11 00:32:47,233] [INFO] [config.py:1005:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2025-03-11 00:32:47,233] [INFO] [config.py:1005:print]   zero_optimization_stage ...... 3\n",
      "[2025-03-11 00:32:47,234] [INFO] [config.py:991:print_user_config]   json = {\n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 3, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"pin_memory\": true\n",
      "        }, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_bucket_size\": \"auto\", \n",
      "        \"stage3_prefetch_bucket_size\": \"auto\", \n",
      "        \"stage3_param_persistence_threshold\": \"auto\", \n",
      "        \"stage3_gather_16bit_weights_on_model_save\": true\n",
      "    }, \n",
      "    \"train_batch_size\": 8\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ParlerTTSForConditionalGeneration(\n",
       "  (text_encoder): T5EncoderModel(\n",
       "    (shared): Embedding(32128, 768)\n",
       "    (encoder): T5Stack(\n",
       "      (embed_tokens): Embedding(32128, 768)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 12)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedActDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1-11): 11 x T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedActDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (audio_encoder): DACModel(\n",
       "    (model): DAC(\n",
       "      (encoder): Encoder(\n",
       "        (block): Sequential(\n",
       "          (0): ParametrizedConv1d(\n",
       "            1, 64, kernel_size=(7,), stride=(1,), padding=(3,)\n",
       "            (parametrizations): ModuleDict(\n",
       "              (weight): ParametrizationList(\n",
       "                (0): _WeightNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1): EncoderBlock(\n",
       "            (block): Sequential(\n",
       "              (0): ResidualUnit(\n",
       "                (block): Sequential(\n",
       "                  (0): Snake1d()\n",
       "                  (1): ParametrizedConv1d(\n",
       "                    64, 64, kernel_size=(7,), stride=(1,), padding=(3,)\n",
       "                    (parametrizations): ModuleDict(\n",
       "                      (weight): ParametrizationList(\n",
       "                        (0): _WeightNorm()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                  (2): Snake1d()\n",
       "                  (3): ParametrizedConv1d(\n",
       "                    64, 64, kernel_size=(1,), stride=(1,)\n",
       "                    (parametrizations): ModuleDict(\n",
       "                      (weight): ParametrizationList(\n",
       "                        (0): _WeightNorm()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): ResidualUnit(\n",
       "                (block): Sequential(\n",
       "                  (0): Snake1d()\n",
       "                  (1): ParametrizedConv1d(\n",
       "                    64, 64, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)\n",
       "                    (parametrizations): ModuleDict(\n",
       "                      (weight): ParametrizationList(\n",
       "                        (0): _WeightNorm()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                  (2): Snake1d()\n",
       "                  (3): ParametrizedConv1d(\n",
       "                    64, 64, kernel_size=(1,), stride=(1,)\n",
       "                    (parametrizations): ModuleDict(\n",
       "                      (weight): ParametrizationList(\n",
       "                        (0): _WeightNorm()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (2): ResidualUnit(\n",
       "                (block): Sequential(\n",
       "                  (0): Snake1d()\n",
       "                  (1): ParametrizedConv1d(\n",
       "                    64, 64, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)\n",
       "                    (parametrizations): ModuleDict(\n",
       "                      (weight): ParametrizationList(\n",
       "                        (0): _WeightNorm()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                  (2): Snake1d()\n",
       "                  (3): ParametrizedConv1d(\n",
       "                    64, 64, kernel_size=(1,), stride=(1,)\n",
       "                    (parametrizations): ModuleDict(\n",
       "                      (weight): ParametrizationList(\n",
       "                        (0): _WeightNorm()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (3): Snake1d()\n",
       "              (4): ParametrizedConv1d(\n",
       "                64, 128, kernel_size=(4,), stride=(2,), padding=(1,)\n",
       "                (parametrizations): ModuleDict(\n",
       "                  (weight): ParametrizationList(\n",
       "                    (0): _WeightNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): EncoderBlock(\n",
       "            (block): Sequential(\n",
       "              (0): ResidualUnit(\n",
       "                (block): Sequential(\n",
       "                  (0): Snake1d()\n",
       "                  (1): ParametrizedConv1d(\n",
       "                    128, 128, kernel_size=(7,), stride=(1,), padding=(3,)\n",
       "                    (parametrizations): ModuleDict(\n",
       "                      (weight): ParametrizationList(\n",
       "                        (0): _WeightNorm()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                  (2): Snake1d()\n",
       "                  (3): ParametrizedConv1d(\n",
       "                    128, 128, kernel_size=(1,), stride=(1,)\n",
       "                    (parametrizations): ModuleDict(\n",
       "                      (weight): ParametrizationList(\n",
       "                        (0): _WeightNorm()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): ResidualUnit(\n",
       "                (block): Sequential(\n",
       "                  (0): Snake1d()\n",
       "                  (1): ParametrizedConv1d(\n",
       "                    128, 128, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)\n",
       "                    (parametrizations): ModuleDict(\n",
       "                      (weight): ParametrizationList(\n",
       "                        (0): _WeightNorm()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                  (2): Snake1d()\n",
       "                  (3): ParametrizedConv1d(\n",
       "                    128, 128, kernel_size=(1,), stride=(1,)\n",
       "                    (parametrizations): ModuleDict(\n",
       "                      (weight): ParametrizationList(\n",
       "                        (0): _WeightNorm()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (2): ResidualUnit(\n",
       "                (block): Sequential(\n",
       "                  (0): Snake1d()\n",
       "                  (1): ParametrizedConv1d(\n",
       "                    128, 128, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)\n",
       "                    (parametrizations): ModuleDict(\n",
       "                      (weight): ParametrizationList(\n",
       "                        (0): _WeightNorm()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                  (2): Snake1d()\n",
       "                  (3): ParametrizedConv1d(\n",
       "                    128, 128, kernel_size=(1,), stride=(1,)\n",
       "                    (parametrizations): ModuleDict(\n",
       "                      (weight): ParametrizationList(\n",
       "                        (0): _WeightNorm()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (3): Snake1d()\n",
       "              (4): ParametrizedConv1d(\n",
       "                128, 256, kernel_size=(8,), stride=(4,), padding=(2,)\n",
       "                (parametrizations): ModuleDict(\n",
       "                  (weight): ParametrizationList(\n",
       "                    (0): _WeightNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (3): EncoderBlock(\n",
       "            (block): Sequential(\n",
       "              (0): ResidualUnit(\n",
       "                (block): Sequential(\n",
       "                  (0): Snake1d()\n",
       "                  (1): ParametrizedConv1d(\n",
       "                    256, 256, kernel_size=(7,), stride=(1,), padding=(3,)\n",
       "                    (parametrizations): ModuleDict(\n",
       "                      (weight): ParametrizationList(\n",
       "                        (0): _WeightNorm()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                  (2): Snake1d()\n",
       "                  (3): ParametrizedConv1d(\n",
       "                    256, 256, kernel_size=(1,), stride=(1,)\n",
       "                    (parametrizations): ModuleDict(\n",
       "                      (weight): ParametrizationList(\n",
       "                        (0): _WeightNorm()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): ResidualUnit(\n",
       "                (block): Sequential(\n",
       "                  (0): Snake1d()\n",
       "                  (1): ParametrizedConv1d(\n",
       "                    256, 256, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)\n",
       "                    (parametrizations): ModuleDict(\n",
       "                      (weight): ParametrizationList(\n",
       "                        (0): _WeightNorm()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                  (2): Snake1d()\n",
       "                  (3): ParametrizedConv1d(\n",
       "                    256, 256, kernel_size=(1,), stride=(1,)\n",
       "                    (parametrizations): ModuleDict(\n",
       "                      (weight): ParametrizationList(\n",
       "                        (0): _WeightNorm()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (2): ResidualUnit(\n",
       "                (block): Sequential(\n",
       "                  (0): Snake1d()\n",
       "                  (1): ParametrizedConv1d(\n",
       "                    256, 256, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)\n",
       "                    (parametrizations): ModuleDict(\n",
       "                      (weight): ParametrizationList(\n",
       "                        (0): _WeightNorm()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                  (2): Snake1d()\n",
       "                  (3): ParametrizedConv1d(\n",
       "                    256, 256, kernel_size=(1,), stride=(1,)\n",
       "                    (parametrizations): ModuleDict(\n",
       "                      (weight): ParametrizationList(\n",
       "                        (0): _WeightNorm()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (3): Snake1d()\n",
       "              (4): ParametrizedConv1d(\n",
       "                256, 512, kernel_size=(16,), stride=(8,), padding=(4,)\n",
       "                (parametrizations): ModuleDict(\n",
       "                  (weight): ParametrizationList(\n",
       "                    (0): _WeightNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (4): EncoderBlock(\n",
       "            (block): Sequential(\n",
       "              (0): ResidualUnit(\n",
       "                (block): Sequential(\n",
       "                  (0): Snake1d()\n",
       "                  (1): ParametrizedConv1d(\n",
       "                    512, 512, kernel_size=(7,), stride=(1,), padding=(3,)\n",
       "                    (parametrizations): ModuleDict(\n",
       "                      (weight): ParametrizationList(\n",
       "                        (0): _WeightNorm()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                  (2): Snake1d()\n",
       "                  (3): ParametrizedConv1d(\n",
       "                    512, 512, kernel_size=(1,), stride=(1,)\n",
       "                    (parametrizations): ModuleDict(\n",
       "                      (weight): ParametrizationList(\n",
       "                        (0): _WeightNorm()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): ResidualUnit(\n",
       "                (block): Sequential(\n",
       "                  (0): Snake1d()\n",
       "                  (1): ParametrizedConv1d(\n",
       "                    512, 512, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)\n",
       "                    (parametrizations): ModuleDict(\n",
       "                      (weight): ParametrizationList(\n",
       "                        (0): _WeightNorm()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                  (2): Snake1d()\n",
       "                  (3): ParametrizedConv1d(\n",
       "                    512, 512, kernel_size=(1,), stride=(1,)\n",
       "                    (parametrizations): ModuleDict(\n",
       "                      (weight): ParametrizationList(\n",
       "                        (0): _WeightNorm()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (2): ResidualUnit(\n",
       "                (block): Sequential(\n",
       "                  (0): Snake1d()\n",
       "                  (1): ParametrizedConv1d(\n",
       "                    512, 512, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)\n",
       "                    (parametrizations): ModuleDict(\n",
       "                      (weight): ParametrizationList(\n",
       "                        (0): _WeightNorm()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                  (2): Snake1d()\n",
       "                  (3): ParametrizedConv1d(\n",
       "                    512, 512, kernel_size=(1,), stride=(1,)\n",
       "                    (parametrizations): ModuleDict(\n",
       "                      (weight): ParametrizationList(\n",
       "                        (0): _WeightNorm()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (3): Snake1d()\n",
       "              (4): ParametrizedConv1d(\n",
       "                512, 1024, kernel_size=(16,), stride=(8,), padding=(4,)\n",
       "                (parametrizations): ModuleDict(\n",
       "                  (weight): ParametrizationList(\n",
       "                    (0): _WeightNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (5): Snake1d()\n",
       "          (6): ParametrizedConv1d(\n",
       "            1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,)\n",
       "            (parametrizations): ModuleDict(\n",
       "              (weight): ParametrizationList(\n",
       "                (0): _WeightNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (quantizer): ResidualVectorQuantize(\n",
       "        (quantizers): ModuleList(\n",
       "          (0-8): 9 x VectorQuantize(\n",
       "            (in_proj): ParametrizedConv1d(\n",
       "              1024, 8, kernel_size=(1,), stride=(1,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (out_proj): ParametrizedConv1d(\n",
       "              8, 1024, kernel_size=(1,), stride=(1,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (codebook): Embedding(1024, 8)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (decoder): Decoder(\n",
       "        (model): Sequential(\n",
       "          (0): ParametrizedConv1d(\n",
       "            1024, 1536, kernel_size=(7,), stride=(1,), padding=(3,)\n",
       "            (parametrizations): ModuleDict(\n",
       "              (weight): ParametrizationList(\n",
       "                (0): _WeightNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1): DecoderBlock(\n",
       "            (block): Sequential(\n",
       "              (0): Snake1d()\n",
       "              (1): ParametrizedConvTranspose1d(\n",
       "                1536, 768, kernel_size=(16,), stride=(8,), padding=(4,)\n",
       "                (parametrizations): ModuleDict(\n",
       "                  (weight): ParametrizationList(\n",
       "                    (0): _WeightNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (2): ResidualUnit(\n",
       "                (block): Sequential(\n",
       "                  (0): Snake1d()\n",
       "                  (1): ParametrizedConv1d(\n",
       "                    768, 768, kernel_size=(7,), stride=(1,), padding=(3,)\n",
       "                    (parametrizations): ModuleDict(\n",
       "                      (weight): ParametrizationList(\n",
       "                        (0): _WeightNorm()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                  (2): Snake1d()\n",
       "                  (3): ParametrizedConv1d(\n",
       "                    768, 768, kernel_size=(1,), stride=(1,)\n",
       "                    (parametrizations): ModuleDict(\n",
       "                      (weight): ParametrizationList(\n",
       "                        (0): _WeightNorm()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (3): ResidualUnit(\n",
       "                (block): Sequential(\n",
       "                  (0): Snake1d()\n",
       "                  (1): ParametrizedConv1d(\n",
       "                    768, 768, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)\n",
       "                    (parametrizations): ModuleDict(\n",
       "                      (weight): ParametrizationList(\n",
       "                        (0): _WeightNorm()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                  (2): Snake1d()\n",
       "                  (3): ParametrizedConv1d(\n",
       "                    768, 768, kernel_size=(1,), stride=(1,)\n",
       "                    (parametrizations): ModuleDict(\n",
       "                      (weight): ParametrizationList(\n",
       "                        (0): _WeightNorm()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (4): ResidualUnit(\n",
       "                (block): Sequential(\n",
       "                  (0): Snake1d()\n",
       "                  (1): ParametrizedConv1d(\n",
       "                    768, 768, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)\n",
       "                    (parametrizations): ModuleDict(\n",
       "                      (weight): ParametrizationList(\n",
       "                        (0): _WeightNorm()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                  (2): Snake1d()\n",
       "                  (3): ParametrizedConv1d(\n",
       "                    768, 768, kernel_size=(1,), stride=(1,)\n",
       "                    (parametrizations): ModuleDict(\n",
       "                      (weight): ParametrizationList(\n",
       "                        (0): _WeightNorm()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): DecoderBlock(\n",
       "            (block): Sequential(\n",
       "              (0): Snake1d()\n",
       "              (1): ParametrizedConvTranspose1d(\n",
       "                768, 384, kernel_size=(16,), stride=(8,), padding=(4,)\n",
       "                (parametrizations): ModuleDict(\n",
       "                  (weight): ParametrizationList(\n",
       "                    (0): _WeightNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (2): ResidualUnit(\n",
       "                (block): Sequential(\n",
       "                  (0): Snake1d()\n",
       "                  (1): ParametrizedConv1d(\n",
       "                    384, 384, kernel_size=(7,), stride=(1,), padding=(3,)\n",
       "                    (parametrizations): ModuleDict(\n",
       "                      (weight): ParametrizationList(\n",
       "                        (0): _WeightNorm()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                  (2): Snake1d()\n",
       "                  (3): ParametrizedConv1d(\n",
       "                    384, 384, kernel_size=(1,), stride=(1,)\n",
       "                    (parametrizations): ModuleDict(\n",
       "                      (weight): ParametrizationList(\n",
       "                        (0): _WeightNorm()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (3): ResidualUnit(\n",
       "                (block): Sequential(\n",
       "                  (0): Snake1d()\n",
       "                  (1): ParametrizedConv1d(\n",
       "                    384, 384, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)\n",
       "                    (parametrizations): ModuleDict(\n",
       "                      (weight): ParametrizationList(\n",
       "                        (0): _WeightNorm()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                  (2): Snake1d()\n",
       "                  (3): ParametrizedConv1d(\n",
       "                    384, 384, kernel_size=(1,), stride=(1,)\n",
       "                    (parametrizations): ModuleDict(\n",
       "                      (weight): ParametrizationList(\n",
       "                        (0): _WeightNorm()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (4): ResidualUnit(\n",
       "                (block): Sequential(\n",
       "                  (0): Snake1d()\n",
       "                  (1): ParametrizedConv1d(\n",
       "                    384, 384, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)\n",
       "                    (parametrizations): ModuleDict(\n",
       "                      (weight): ParametrizationList(\n",
       "                        (0): _WeightNorm()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                  (2): Snake1d()\n",
       "                  (3): ParametrizedConv1d(\n",
       "                    384, 384, kernel_size=(1,), stride=(1,)\n",
       "                    (parametrizations): ModuleDict(\n",
       "                      (weight): ParametrizationList(\n",
       "                        (0): _WeightNorm()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (3): DecoderBlock(\n",
       "            (block): Sequential(\n",
       "              (0): Snake1d()\n",
       "              (1): ParametrizedConvTranspose1d(\n",
       "                384, 192, kernel_size=(8,), stride=(4,), padding=(2,)\n",
       "                (parametrizations): ModuleDict(\n",
       "                  (weight): ParametrizationList(\n",
       "                    (0): _WeightNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (2): ResidualUnit(\n",
       "                (block): Sequential(\n",
       "                  (0): Snake1d()\n",
       "                  (1): ParametrizedConv1d(\n",
       "                    192, 192, kernel_size=(7,), stride=(1,), padding=(3,)\n",
       "                    (parametrizations): ModuleDict(\n",
       "                      (weight): ParametrizationList(\n",
       "                        (0): _WeightNorm()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                  (2): Snake1d()\n",
       "                  (3): ParametrizedConv1d(\n",
       "                    192, 192, kernel_size=(1,), stride=(1,)\n",
       "                    (parametrizations): ModuleDict(\n",
       "                      (weight): ParametrizationList(\n",
       "                        (0): _WeightNorm()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (3): ResidualUnit(\n",
       "                (block): Sequential(\n",
       "                  (0): Snake1d()\n",
       "                  (1): ParametrizedConv1d(\n",
       "                    192, 192, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)\n",
       "                    (parametrizations): ModuleDict(\n",
       "                      (weight): ParametrizationList(\n",
       "                        (0): _WeightNorm()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                  (2): Snake1d()\n",
       "                  (3): ParametrizedConv1d(\n",
       "                    192, 192, kernel_size=(1,), stride=(1,)\n",
       "                    (parametrizations): ModuleDict(\n",
       "                      (weight): ParametrizationList(\n",
       "                        (0): _WeightNorm()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (4): ResidualUnit(\n",
       "                (block): Sequential(\n",
       "                  (0): Snake1d()\n",
       "                  (1): ParametrizedConv1d(\n",
       "                    192, 192, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)\n",
       "                    (parametrizations): ModuleDict(\n",
       "                      (weight): ParametrizationList(\n",
       "                        (0): _WeightNorm()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                  (2): Snake1d()\n",
       "                  (3): ParametrizedConv1d(\n",
       "                    192, 192, kernel_size=(1,), stride=(1,)\n",
       "                    (parametrizations): ModuleDict(\n",
       "                      (weight): ParametrizationList(\n",
       "                        (0): _WeightNorm()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (4): DecoderBlock(\n",
       "            (block): Sequential(\n",
       "              (0): Snake1d()\n",
       "              (1): ParametrizedConvTranspose1d(\n",
       "                192, 96, kernel_size=(4,), stride=(2,), padding=(1,)\n",
       "                (parametrizations): ModuleDict(\n",
       "                  (weight): ParametrizationList(\n",
       "                    (0): _WeightNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (2): ResidualUnit(\n",
       "                (block): Sequential(\n",
       "                  (0): Snake1d()\n",
       "                  (1): ParametrizedConv1d(\n",
       "                    96, 96, kernel_size=(7,), stride=(1,), padding=(3,)\n",
       "                    (parametrizations): ModuleDict(\n",
       "                      (weight): ParametrizationList(\n",
       "                        (0): _WeightNorm()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                  (2): Snake1d()\n",
       "                  (3): ParametrizedConv1d(\n",
       "                    96, 96, kernel_size=(1,), stride=(1,)\n",
       "                    (parametrizations): ModuleDict(\n",
       "                      (weight): ParametrizationList(\n",
       "                        (0): _WeightNorm()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (3): ResidualUnit(\n",
       "                (block): Sequential(\n",
       "                  (0): Snake1d()\n",
       "                  (1): ParametrizedConv1d(\n",
       "                    96, 96, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)\n",
       "                    (parametrizations): ModuleDict(\n",
       "                      (weight): ParametrizationList(\n",
       "                        (0): _WeightNorm()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                  (2): Snake1d()\n",
       "                  (3): ParametrizedConv1d(\n",
       "                    96, 96, kernel_size=(1,), stride=(1,)\n",
       "                    (parametrizations): ModuleDict(\n",
       "                      (weight): ParametrizationList(\n",
       "                        (0): _WeightNorm()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (4): ResidualUnit(\n",
       "                (block): Sequential(\n",
       "                  (0): Snake1d()\n",
       "                  (1): ParametrizedConv1d(\n",
       "                    96, 96, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)\n",
       "                    (parametrizations): ModuleDict(\n",
       "                      (weight): ParametrizationList(\n",
       "                        (0): _WeightNorm()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                  (2): Snake1d()\n",
       "                  (3): ParametrizedConv1d(\n",
       "                    96, 96, kernel_size=(1,), stride=(1,)\n",
       "                    (parametrizations): ModuleDict(\n",
       "                      (weight): ParametrizationList(\n",
       "                        (0): _WeightNorm()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (5): Snake1d()\n",
       "          (6): ParametrizedConv1d(\n",
       "            96, 1, kernel_size=(7,), stride=(1,), padding=(3,)\n",
       "            (parametrizations): ModuleDict(\n",
       "              (weight): ParametrizationList(\n",
       "                (0): _WeightNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (7): Tanh()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): ParlerTTSForCausalLM(\n",
       "    (model): ParlerTTSModel(\n",
       "      (decoder): ParlerTTSDecoder(\n",
       "        (embed_tokens): ModuleList(\n",
       "          (0-8): 9 x Embedding(1089, 1024)\n",
       "        )\n",
       "        (embed_positions): ParlerTTSSinusoidalPositionalEmbedding()\n",
       "        (layers): ModuleList(\n",
       "          (0-23): 24 x ParlerTTSDecoderLayer(\n",
       "            (self_attn): ParlerTTSSdpaAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (activation_fn): GELUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): ParlerTTSSdpaAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (lm_heads): ModuleList(\n",
       "      (0-8): 9 x Linear(in_features=1024, out_features=1088, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (enc_to_dec_proj): Linear(in_features=768, out_features=1024, bias=True)\n",
       "  (embed_prompts): Embedding(32128, 1024)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_engine = deepspeed.initialize(model=model, config_params=ds_config)[0]\n",
    "ds_engine.module.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = ds_engine.module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "'weight' must be 2-D",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m generation \u001b[38;5;241m=\u001b[39m \u001b[43mnew_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_input_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_input_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     16\u001b[0m execution_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m/home/system4/anaconda3/envs/tf-gpu-211/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Pavan/vivek/assign_1/parler-tts/parler_tts/modeling_parler_tts.py:3637\u001b[0m, in \u001b[0;36mParlerTTSForConditionalGeneration.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, synced_gpus, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   3635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_mask\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   3636\u001b[0m     sample \u001b[38;5;241m=\u001b[39m sample[:, :, sample_mask] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_4dim_audio_codes \u001b[38;5;28;01melse\u001b[39;00m sample[:, sample_mask]\n\u001b[0;32m-> 3637\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maudio_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_codes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msingle_audio_decode_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39maudio_values\n\u001b[1;32m   3638\u001b[0m     sample \u001b[38;5;241m=\u001b[39m sample \u001b[38;5;28;01mif\u001b[39;00m sample\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m sample\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   3639\u001b[0m     output_values\u001b[38;5;241m.\u001b[39mappend(sample\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n",
      "File \u001b[0;32m~/Pavan/vivek/assign_1/parler-tts/parler_tts/dac_wrapper/modeling_dac.py:138\u001b[0m, in \u001b[0;36mDACModel.decode\u001b[0;34m(self, audio_codes, audio_scales, padding_mask, return_dict)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(audio_codes) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected one frame, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(audio_codes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 138\u001b[0m audio_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_codes\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_codes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    139\u001b[0m audio_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdecode(audio_values)\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n",
      "File \u001b[0;32m/home/system4/anaconda3/envs/tf-gpu-211/lib/python3.10/site-packages/dac/nn/quantize.py:215\u001b[0m, in \u001b[0;36mResidualVectorQuantize.from_codes\u001b[0;34m(self, codes)\u001b[0m\n\u001b[1;32m    213\u001b[0m n_codebooks \u001b[38;5;241m=\u001b[39m codes\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_codebooks):\n\u001b[0;32m--> 215\u001b[0m     z_p_i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantizers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_code\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcodes\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     z_p\u001b[38;5;241m.\u001b[39mappend(z_p_i)\n\u001b[1;32m    218\u001b[0m     z_q_i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquantizers[i]\u001b[38;5;241m.\u001b[39mout_proj(z_p_i)\n",
      "File \u001b[0;32m/home/system4/anaconda3/envs/tf-gpu-211/lib/python3.10/site-packages/dac/nn/quantize.py:76\u001b[0m, in \u001b[0;36mVectorQuantize.decode_code\u001b[0;34m(self, embed_id)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_code\u001b[39m(\u001b[38;5;28mself\u001b[39m, embed_id):\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_code\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_id\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m/home/system4/anaconda3/envs/tf-gpu-211/lib/python3.10/site-packages/dac/nn/quantize.py:73\u001b[0m, in \u001b[0;36mVectorQuantize.embed_code\u001b[0;34m(self, embed_id)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed_code\u001b[39m(\u001b[38;5;28mself\u001b[39m, embed_id):\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcodebook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/system4/anaconda3/envs/tf-gpu-211/lib/python3.10/site-packages/torch/nn/functional.py:2551\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2545\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2547\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2548\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2549\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2550\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: 'weight' must be 2-D"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "times = []\n",
    "for i in range(5):\n",
    "    start_time = time.time()\n",
    "    # with torch.no_grad():\n",
    "    generation = new_model.generate(\n",
    "        input_ids=input_ids.input_ids,\n",
    "        attention_mask=input_ids.attention_mask,\n",
    "        prompt_input_ids=prompt_input_ids.input_ids,\n",
    "        prompt_attention_mask=prompt_input_ids.attention_mask,\n",
    "        do_sample=True,\n",
    "        use_cache=True,\n",
    "        return_dict_in_generate=True)\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    times.append(execution_time)\n",
    "    print(f\"Iteration {i+1} took {execution_time:.2f} seconds\")\n",
    "\n",
    "avg_time = sum(times) / len(times)\n",
    "print(f\"\\nAverage execution time: {avg_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu-211",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
