{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this notebook, we will analyze and identify the key bottlenecks affecting the model's latency and throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flash attention 2 is not installed\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from parler_tts import ParlerTTSForConditionalGeneration \n",
    "from transformers import AutoTokenizer\n",
    "import soundfile as sf\n",
    "import time\n",
    "from torch.profiler import profile, record_function, ProfilerActivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Hello world.\",\n",
    "    \"A futuristic AI assistant responding in a clear, robotic yet friendly manner\",\n",
    "    \"A dramatic voice-over for an action-packed movie trailer that builds suspense\",\n",
    "    \"A casual and friendly conversation starter that feels natural and engaging\",\n",
    "    \"An enchanting introduction to a bedtime story that sparks imagination\",\n",
    "    \"A detailed weather update that provides temperature, wind speed, and overall forecast\",\n",
    "    \"A robotic voice delivering a monotone yet precise system status update\",\n",
    "    \"A powerful and energetic motivational speech that inspires action and confidence\"\n",
    "]\n",
    "\n",
    "descriptions = [\n",
    "    \"A female speaker delivers a warm and welcoming message with a slightly expressive and friendly tone. The speech has a moderate pace and a natural intonation, making it feel inviting. The recording is clear, with minimal background noise.\",\n",
    "    \"A robotic AI voice speaks in a neutral yet slightly friendly manner. The speech is steady, with a consistent pitch and minimal variation in tone. The audio is crisp, resembling a synthesized assistant's response.\",\n",
    "    \"A deep male voice narrates with a dramatic and intense tone, building suspense with pauses and rising intensity. The speech is slow-paced, emphasizing key moments. The recording is cinematic, with a slight reverberation for a grand effect.\",\n",
    "    \"A young adult male speaker talks in a casual and relaxed tone. The speech is natural, with slight variations in pitch and pauses that mimic real-life conversations. The recording is high quality, making it feel like a personal chat.\",\n",
    "    \"A soft-spoken female speaker introduces a bedtime story with a soothing and melodic tone. The pace is slow and gentle, making it easy to follow. The recording has a slight warmth, resembling a close-up microphone capture.\",\n",
    "    \"A professional male voice delivers a clear and informative weather update. The tone is neutral but engaging, with a moderate pace. The articulation is precise, and the recording is high quality, with no distortions.\",\n",
    "    \"A synthetic robotic voice speaks in a completely monotone and even-paced manner. The pitch remains constant, with no emotional inflection. The recording is crisp and clean, resembling an automated system response.\",\n",
    "    \"A dynamic and energetic male speaker delivers an inspiring speech with a strong, enthusiastic tone. The pace is varied, with emphasis on key words to motivate the listener. The recording is sharp and immersive, with no background noise.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/system4/anaconda3/envs/tf-gpu-211/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "Config of the text_encoder: <class 'transformers.models.t5.modeling_t5.T5EncoderModel'> is overwritten by shared text_encoder config: T5Config {\n",
      "  \"_name_or_path\": \"google/flan-t5-base\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "Config of the audio_encoder: <class 'parler_tts.dac_wrapper.modeling_dac.DACModel'> is overwritten by shared audio_encoder config: DACConfig {\n",
      "  \"_name_or_path\": \"ylacombe/dac_44khZ_8kbps\",\n",
      "  \"architectures\": [\n",
      "    \"DACModel\"\n",
      "  ],\n",
      "  \"codebook_size\": 1024,\n",
      "  \"frame_rate\": 86,\n",
      "  \"latent_dim\": 1024,\n",
      "  \"model_bitrate\": 8,\n",
      "  \"model_type\": \"dac_on_the_hub\",\n",
      "  \"num_codebooks\": 9,\n",
      "  \"sampling_rate\": 44100,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'parler_tts.modeling_parler_tts.ParlerTTSForCausalLM'> is overwritten by shared decoder config: ParlerTTSDecoderConfig {\n",
      "  \"_name_or_path\": \"/fsx/yoach/tmp/artefacts/decoder_400M/\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"architectures\": [\n",
      "    \"ParlerTTSForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1025,\n",
      "  \"codebook_weights\": null,\n",
      "  \"cross_attention_implementation_strategy\": null,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 1024,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_factor\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"parler_tts_decoder\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codebooks\": 9,\n",
      "  \"num_cross_attention_key_value_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"pad_token_id\": 1024,\n",
      "  \"rope_embeddings\": false,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"scale_embedding\": false,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_fused_lm_heads\": false,\n",
      "  \"vocab_size\": 1088\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = ParlerTTSForConditionalGeneration.from_pretrained(\"parler-tts/parler_tts_mini_v0.1\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"parler-tts/parler_tts_mini_v0.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_audio_with_profiling(model, tokenizer, prompt, description, output_file=\"parler_tts_out.wav\"):\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "                profile_memory=True, record_shapes=True) as prof:\n",
    "        \n",
    "        with record_function(\"tokenization\"):\n",
    "            input_ids = tokenizer(description, return_tensors=\"pt\").to(device)\n",
    "            prompt_input_ids = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        with record_function(\"generation\"):\n",
    "            generation = model.generate(input_ids=input_ids.input_ids,\n",
    "                attention_mask=input_ids.attention_mask,\n",
    "                prompt_input_ids=prompt_input_ids.input_ids,\n",
    "                prompt_attention_mask=prompt_input_ids.attention_mask)\n",
    "            audio_arr = generation.cpu().numpy().squeeze()\n",
    "        \n",
    "        sf.write(output_file, audio_arr, model.config.sampling_rate)\n",
    "\n",
    "    print(\"\\nCUDA and CPU Usage:\")\n",
    "    print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "    print(\"\\nMemory Usage:\")\n",
    "    print(prof.key_averages().table(sort_by=\"self_cuda_memory_usage\", row_limit=10))\n",
    "    \n",
    "    return audio_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`prompt_attention_mask` is specified but `attention_mask` is not. A full `attention_mask` will be created. Make sure this is the intended behaviour.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CUDA and CPU Usage:\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                             generation         0.00%       0.000us         0.00%       0.000us       0.000us        4.355s      1202.27%        4.355s        2.177s           0 b           0 b           0 b           0 b             2  \n",
      "                                             generation        27.56%        1.202s        99.86%        4.357s        4.357s       0.000us         0.00%     362.222ms     362.222ms     162.00 Kb    -119.30 Kb       8.28 Mb      -2.03 Gb             1  \n",
      "                                           aten::matmul         2.92%     127.580ms        18.73%     817.360ms      44.797us       0.000us         0.00%     227.084ms      12.446us           0 b           0 b     139.45 Mb           0 b         18246  \n",
      "                                           aten::linear         1.27%      55.224ms        22.62%     986.653ms      54.143us       0.000us         0.00%     226.823ms      12.447us           0 b           0 b     136.51 Mb           0 b         18223  \n",
      "                                               aten::mm        10.37%     452.590ms        13.90%     606.632ms      33.291us     226.807ms        62.62%     226.807ms      12.447us           0 b           0 b     136.31 Mb     136.31 Mb         18222  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us     166.286ms        45.91%     166.286ms      10.556us           0 b           0 b           0 b           0 b         15753  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us      54.383ms        15.01%      54.383ms      25.460us           0 b           0 b           0 b           0 b          2136  \n",
      "                     aten::scaled_dot_product_attention         1.21%      52.692ms         7.41%     323.083ms      74.788us       0.000us         0.00%      53.776ms      12.448us       1.88 Kb     -65.62 Kb      17.62 Mb           0 b          4320  \n",
      "          aten::_scaled_dot_product_efficient_attention         1.06%      46.423ms         6.20%     270.390ms      62.590us       0.000us         0.00%      53.776ms      12.448us      67.50 Kb           0 b      17.62 Mb           0 b          4320  \n",
      "                     aten::_efficient_attention_forward         1.62%      70.486ms         3.93%     171.654ms      39.735us      53.776ms        14.85%      53.776ms      12.448us      67.50 Kb           0 b      17.62 Mb           0 b          4320  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 4.363s\n",
      "Self CUDA time total: 362.224ms\n",
      "\n",
      "\n",
      "Memory Usage:\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                              aten::cat         1.69%      73.712ms         2.53%     110.583ms      24.331us       9.937ms         2.74%       9.937ms       2.186us           0 b           0 b     838.34 Mb     838.34 Mb          4545  \n",
      "                                            aten::empty         4.04%     176.354ms         4.07%     177.407ms       4.460us       0.000us         0.00%       0.000us       0.000us     115.33 Kb     115.33 Kb     277.53 Mb     277.53 Mb         39778  \n",
      "               fused_add_reciprocal_mul_sin_pow_mul_add         0.02%     795.461us         0.02%       1.031ms      39.650us     832.484us         0.23%     832.484us      32.019us           0 b           0 b     270.28 Mb     270.28 Mb            26  \n",
      "                                aten::cudnn_convolution         2.42%     105.691ms         3.62%     157.903ms       4.512ms       3.455ms         0.95%       3.455ms      98.727us           0 b           0 b     242.68 Mb     242.68 Mb            35  \n",
      "                                              aten::add         2.66%     115.846ms         3.92%     170.810ms      21.488us      10.962ms         3.03%      10.962ms       1.379us      23.48 Kb      23.48 Kb     166.54 Mb     166.54 Mb          7949  \n",
      "                                               aten::mm        10.37%     452.590ms        13.90%     606.632ms      33.291us     226.807ms        62.62%     226.807ms      12.447us           0 b           0 b     136.31 Mb     136.31 Mb         18222  \n",
      "                                              aten::mul         0.09%       3.896ms         0.15%       6.334ms      20.365us     600.014us         0.17%     600.014us       1.929us           0 b           0 b      43.77 Mb      43.77 Mb           311  \n",
      "                      aten::cudnn_convolution_transpose         0.06%       2.561ms         0.40%      17.312ms       4.328ms     895.907us         0.25%     895.907us     223.977us           0 b           0 b      40.00 Mb      40.00 Mb             4  \n",
      "                                             aten::gelu         0.70%      30.464ms         1.06%      46.357ms      21.462us       3.022ms         0.83%       3.022ms       1.399us           0 b           0 b      35.25 Mb      35.25 Mb          2160  \n",
      "                                    aten::empty_strided         0.11%       4.715ms         0.11%       4.715ms       7.820us       0.000us         0.00%       0.000us       0.000us     162.00 Kb     162.00 Kb      14.12 Mb      14.12 Mb           603  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 4.363s\n",
      "Self CUDA time total: 362.224ms\n",
      "\n",
      "\n",
      "CUDA and CPU Usage:\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                             generation         0.00%       0.000us         0.00%       0.000us       0.000us       14.412s       782.88%       14.412s       14.412s           0 b           0 b           0 b           0 b             1  \n",
      "                                             generation        31.54%        4.547s        99.97%       14.413s       14.413s       0.000us         0.00%        1.841s        1.841s     792.00 Kb    -143.46 Kb     792.00 Kb     -21.75 Gb             1  \n",
      "                                           aten::matmul         3.67%     529.764ms        22.49%        3.242s      39.752us       0.000us         0.00%        1.032s      12.650us           0 b           0 b     482.08 Mb           0 b         81561  \n",
      "                                           aten::linear         1.54%     221.492ms        27.38%        3.947s      48.406us       0.000us         0.00%        1.031s      12.650us           0 b           0 b     479.06 Mb           0 b         81538  \n",
      "                                               aten::mm        12.36%        1.783s        16.46%        2.373s      29.102us        1.031s        56.03%        1.031s      12.650us           0 b           0 b     478.86 Mb     478.86 Mb         81537  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us     772.119ms        41.94%     772.119ms      10.798us           0 b           0 b           0 b           0 b         71508  \n",
      "                     aten::scaled_dot_product_attention         1.60%     230.867ms         9.67%        1.394s      71.714us       0.000us         0.00%     446.174ms      22.951us      12.20 Kb    -291.55 Kb      78.94 Mb           0 b         19440  \n",
      "          aten::_scaled_dot_product_efficient_attention         1.35%     195.120ms         8.07%        1.163s      59.838us       0.000us         0.00%     446.174ms      22.951us     303.75 Kb           0 b      78.94 Mb           0 b         19440  \n",
      "                     aten::_efficient_attention_forward         2.15%     310.229ms         5.19%     748.390ms      38.497us     446.174ms        24.24%     446.174ms      22.951us     303.75 Kb         304 b      78.94 Mb           0 b         19440  \n",
      "fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEf...         0.00%       0.000us         0.00%       0.000us       0.000us     446.174ms        24.24%     446.174ms      22.951us           0 b           0 b           0 b           0 b         19440  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 14.417s\n",
      "Self CUDA time total: 1.841s\n",
      "\n",
      "\n",
      "Memory Usage:\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                              aten::cat         2.22%     320.778ms         3.29%     474.466ms      23.021us      71.224ms         3.87%      71.224ms       3.456us           0 b           0 b      17.12 Gb      17.12 Gb         20610  \n",
      "               fused_add_reciprocal_mul_sin_pow_mul_add         0.01%     962.483us         0.01%       1.193ms      41.140us       4.016ms         0.22%       4.016ms     138.482us           0 b           0 b       1.33 Gb       1.33 Gb            29  \n",
      "                                aten::cudnn_convolution         0.18%      26.162ms         0.47%      67.130ms       1.918ms      13.299ms         0.72%      13.299ms     379.977us           0 b           0 b       1.16 Gb       1.16 Gb            35  \n",
      "                                              aten::add         3.17%     457.165ms         4.89%     705.068ms      19.946us      49.440ms         2.69%      49.440ms       1.399us      26.24 Kb      26.24 Kb     748.10 Mb     748.10 Mb         35348  \n",
      "                                               aten::mm        12.36%        1.783s        16.46%        2.373s      29.102us        1.031s        56.03%        1.031s      12.650us           0 b           0 b     478.86 Mb     478.86 Mb         81537  \n",
      "                                            aten::empty         5.36%     772.196ms         5.36%     772.196ms       4.352us       0.000us         0.00%       0.000us       0.000us     357.03 Kb     357.03 Kb     474.23 Mb     474.23 Mb        177433  \n",
      "                      aten::cudnn_convolution_transpose         0.01%       1.919ms         0.05%       6.673ms       1.668ms       3.443ms         0.19%       3.443ms     860.642us           0 b           0 b     194.91 Mb     194.91 Mb             4  \n",
      "                                             aten::gelu         0.88%     127.085ms         1.33%     191.400ms      19.691us      13.604ms         0.74%      13.604ms       1.400us           0 b           0 b     157.88 Mb     157.88 Mb          9720  \n",
      "                                    aten::empty_strided         0.11%      16.488ms         0.11%      16.488ms       6.614us       0.000us         0.00%       0.000us       0.000us     792.00 Kb     792.00 Kb      67.12 Mb      67.12 Mb          2493  \n",
      "                                          aten::resize_         0.19%      27.470ms         0.19%      27.470ms       3.914us       0.000us         0.00%       0.000us       0.000us           0 b           0 b      33.55 Mb      33.55 Mb          7018  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 14.417s\n",
      "Self CUDA time total: 1.841s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    generate_audio_with_profiling(model, tokenizer, prompts[i], descriptions[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate RTFx and Latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_performance(model1, tokenizer, prompt, description):\n",
    "    try:\n",
    "\n",
    "        prompt_input_ids = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        input_ids = tokenizer(description, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "\n",
    "            audio = model1.generate(input_ids=input_ids.input_ids,\n",
    "                attention_mask=input_ids.attention_mask,\n",
    "                prompt_input_ids=prompt_input_ids.input_ids,\n",
    "                prompt_attention_mask=prompt_input_ids.attention_mask) \n",
    "        end_time = time.time()\n",
    "        \n",
    "        \n",
    "        generation_time = end_time - start_time\n",
    "        audio_length = audio.shape[-1] / model.config.sampling_rate\n",
    "        rtfx = generation_time / audio_length\n",
    "        \n",
    "        return generation_time, audio_length, rtfx\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing prompt: {e}\")\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt length: 12 characters\n",
      "Generation time: 2.56 seconds\n",
      "Audio length: 1.17 seconds\n",
      "RTFx: 2.18\n",
      "\n",
      "Prompt length: 76 characters\n",
      "Generation time: 9.35 seconds\n",
      "Audio length: 4.78 seconds\n",
      "RTFx: 1.96\n",
      "\n",
      "Prompt length: 77 characters\n",
      "Generation time: 9.61 seconds\n",
      "Audio length: 4.89 seconds\n",
      "RTFx: 1.97\n",
      "\n",
      "Prompt length: 74 characters\n",
      "Generation time: 10.14 seconds\n",
      "Audio length: 5.15 seconds\n",
      "RTFx: 1.97\n",
      "\n",
      "Prompt length: 69 characters\n",
      "Generation time: 9.52 seconds\n",
      "Audio length: 4.81 seconds\n",
      "RTFx: 1.98\n",
      "\n",
      "Prompt length: 85 characters\n",
      "Generation time: 7.64 seconds\n",
      "Audio length: 3.77 seconds\n",
      "RTFx: 2.02\n",
      "\n",
      "Prompt length: 70 characters\n",
      "Generation time: 8.24 seconds\n",
      "Audio length: 4.02 seconds\n",
      "RTFx: 2.05\n",
      "\n",
      "Prompt length: 80 characters\n",
      "Generation time: 9.79 seconds\n",
      "Audio length: 4.88 seconds\n",
      "RTFx: 2.01\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for i in range(len(prompts)):\n",
    "    prompt, description = prompts[i], descriptions[i]\n",
    "    generation_time, audio_length, rtfx = measure_performance(model, tokenizer, prompt, description)\n",
    "    if generation_time is not None:\n",
    "        results.append({\n",
    "            'prompt_length': len(prompt),\n",
    "            'generation_time': generation_time,\n",
    "            'audio_length': audio_length,\n",
    "            'rtfx': rtfx\n",
    "        })\n",
    "        print(f\"\\nPrompt length: {len(prompt)} characters\")\n",
    "        print(f\"Generation time: {generation_time:.2f} seconds\")\n",
    "        print(f\"Audio length: {audio_length:.2f} seconds\") \n",
    "        print(f\"RTFx: {rtfx:.2f}\")\n",
    "    else:\n",
    "        print(f\"\\nFailed to process prompt: {prompt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(descriptions, return_tensors=\"pt\", padding=\"max_length\",max_length=80).to(device)\n",
    "prompt_input_ids = tokenizer(prompts, return_tensors=\"pt\", padding=\"max_length\",max_length=80).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation when batching is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation time: 18.75 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    audio = model.generate(input_ids=input_ids.input_ids,\n",
    "        attention_mask=input_ids.attention_mask,\n",
    "        prompt_input_ids=prompt_input_ids.input_ids,\n",
    "        prompt_attention_mask=prompt_input_ids.attention_mask) \n",
    "end_time = time.time()\n",
    "generation_time = end_time - start_time\n",
    "print(f\"Generation time: {generation_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"generated_audio.wav\"\n",
    "audio_arr = audio[1].cpu().to(torch.float32).numpy().squeeze()\n",
    "sf.write(output_file, audio_arr, model.config.sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When we increase the num of tokens, the generation time increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation time: 24.57 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    audio = model.generate(input_ids=input_ids.input_ids,\n",
    "        attention_mask=input_ids.attention_mask,\n",
    "        prompt_input_ids=prompt_input_ids.input_ids,\n",
    "        prompt_attention_mask=prompt_input_ids.attention_mask,\n",
    "        min_new_tokens=100) \n",
    "end_time = time.time()\n",
    "generation_time = end_time - start_time\n",
    "print(f\"Generation time: {generation_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"generated_audio.wav\"\n",
    "audio_arr = audio[1].cpu().to(torch.float32).numpy().squeeze()\n",
    "sf.write(output_file, audio_arr, model.config.sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synchronize the GPU, When we increase the num of tokens, the generation time increases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation time: 19.78 seconds\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.synchronize()\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    audio = model.generate(input_ids=input_ids.input_ids,\n",
    "        attention_mask=input_ids.attention_mask,\n",
    "        prompt_input_ids=prompt_input_ids.input_ids,\n",
    "        prompt_attention_mask=prompt_input_ids.attention_mask,\n",
    "        min_new_tokens=100) \n",
    "end_time = time.time()\n",
    "torch.cuda.synchronize()\n",
    "generation_time = end_time - start_time\n",
    "print(f\"Generation time: {generation_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"generated_audio.wav\"\n",
    "audio_arr = audio[1].cpu().to(torch.float32).numpy().squeeze()\n",
    "sf.write(output_file, audio_arr, model.config.sampling_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu-211",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
